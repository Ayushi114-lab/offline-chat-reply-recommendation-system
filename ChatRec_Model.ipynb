# ChatRec_Model.ipynb
# Offline Chat-Reply Recommendation System

import os
import math
import random
from pathlib import Path
from typing import List, Dict, Tuple

import numpy as np
import pandas as pd
import torch
from torch.utils.data import Dataset
from transformers import (
    T5ForConditionalGeneration,
    T5TokenizerFast,
    Trainer,
    TrainingArguments,
    DataCollatorForSeq2Seq,
)
import nltk
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
try:
    from rouge_score import rouge_scorer
    _HAS_ROUGE = True
except Exception:
    _HAS_ROUGE = False
import joblib

RANDOM_SEED = 42
random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)
torch.manual_seed(RANDOM_SEED)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

DATA_PATH_A = "/Desktop/Dataset/userA_chats.csv"
DATA_PATH_B = "/Desktop/Dataset/userB_chats.csv"
OUTPUT_DIR = "./chatrec_output"
os.makedirs(OUTPUT_DIR, exist_ok=True)

MODEL_NAME = "t5-small"
MAX_INPUT_LENGTH = 256
MAX_TARGET_LENGTH = 64
BATCH_SIZE = 4
NUM_EPOCHS = 3
LEARNING_RATE = 5e-5


def safe_read_csv(path: str) -> pd.DataFrame:
    if not os.path.exists(path):
        raise FileNotFoundError(f"Expected dataset at {path}.")
    return pd.read_csv(path)


print("Loading datasets...")
try:
    a_df = safe_read_csv(DATA_PATH_A)
    b_df = safe_read_csv(DATA_PATH_B)
except FileNotFoundError:
    a_df = pd.DataFrame({"timestamp": [1, 3, 5, 7], "message": ["Hi!", "I'm good, thanks.", "Let's meet tomorrow.", "Great, see you!"]})
    b_df = pd.DataFrame({"timestamp": [2, 4, 6], "message": ["Hello!", "How are you?", "Are we still on for tomorrow?"]})

for df in [a_df, b_df]:
    if 'message' not in df.columns:
        for c in ['text', 'content', 'msg', 'Message']:
            if c in df.columns:
                df.rename(columns={c: 'message'}, inplace=True)
                break
    if 'timestamp' not in df.columns:
        for c in ['time', 'created_at', 'date']:
            if c in df.columns:
                df.rename(columns={c: 'timestamp'}, inplace=True)
                break

a_df['user'] = 'A'
b_df['user'] = 'B'
if 'timestamp' not in a_df.columns or 'timestamp' not in b_df.columns:
    a_df['timestamp'] = range(0, 2 * len(a_df), 2)
    b_df['timestamp'] = range(1, 2 * len(b_df) + 1, 2)

chat_df = pd.concat([a_df, b_df], ignore_index=True).sort_values('timestamp').reset_index(drop=True)

CONTEXT_TURNS = 6
pairs: List[Tuple[str, str]] = []
for i in range(len(chat_df) - 1):
    if chat_df.loc[i, 'user'] == 'B' and chat_df.loc[i + 1, 'user'] == 'A':
        start = max(0, i - CONTEXT_TURNS + 1)
        context_msgs = chat_df.loc[start:i, ['user', 'message']]
        context_text = " ".join([f"[{r.user}] {r.message}" for r in context_msgs.itertuples(index=False)])
        response_text = chat_df.loc[i + 1, 'message']
        pairs.append((context_text, response_text))

if len(pairs) == 0:
    pairs = [('[B] Hello! [A] Hi! [B] How are you?', "I'm fine, thanks for asking."), ('[B] Are you coming tomorrow?', 'Yes â€” I will be there at 10am.')]

pairs_df = pd.DataFrame(pairs, columns=['context', 'response'])

tokenizer = T5TokenizerFast.from_pretrained(MODEL_NAME)
model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)
model.to(device)


def preprocess_pairs(pairs_df: pd.DataFrame, tokenizer: T5TokenizerFast, max_input_len: int = MAX_INPUT_LENGTH, max_target_len: int = MAX_TARGET_LENGTH) -> Dict[str, torch.Tensor]:
    inputs = tokenizer(list(pairs_df['context'].astype(str)), padding=True, truncation=True, max_length=max_input_len, return_tensors='pt')
    targets = tokenizer(list(pairs_df['response'].astype(str)), padding=True, truncation=True, max_length=max_target_len, return_tensors='pt')
    labels = targets['input_ids'].clone()
    labels[labels == tokenizer.pad_token_id] = -100
    return {'input_ids': inputs['input_ids'], 'attention_mask': inputs['attention_mask'], 'labels': labels}


prepared = preprocess_pairs(pairs_df, tokenizer)

class ChatRecDataset(Dataset):
    def _init_(self, encodings: Dict[str, torch.Tensor]):
        self.encodings = encodings

    def _len_(self):
        return self.encodings['input_ids'].size(0)

    def _getitem_(self, idx):
        return {k: v[idx] for k, v in self.encodings.items()}

train_dataset = ChatRecDataset(prepared)

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

training_args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    num_train_epochs=NUM_EPOCHS,
    per_device_train_batch_size=BATCH_SIZE,
    learning_rate=LEARNING_RATE,
    weight_decay=0.01,
    logging_dir=os.path.join(OUTPUT_DIR, 'logs'),
    logging_steps=10,
    save_total_limit=1,
    fp16=False if device.type == 'cpu' else True,
    remove_unused_columns=False,
)

trainer = Trainer(model=model, args=training_args, train_dataset=train_dataset, data_collator=data_collator)
trainer.train()

model_save_path = os.path.join(OUTPUT_DIR, 't5_chatrec_model')
model.save_pretrained(model_save_path)
tokenizer.save_pretrained(model_save_path)

joblib.dump({'model_dir': model_save_path, 'model_name': MODEL_NAME}, os.path.join(OUTPUT_DIR, 'Model.joblib'))

def generate_reply(context: str, model: T5ForConditionalGeneration, tokenizer: T5TokenizerFast, max_length: int = MAX_TARGET_LENGTH, num_beams: int = 4) -> str:
    model.eval()
    inputs = tokenizer(context, return_tensors='pt', truncation=True, padding=True, max_length=MAX_INPUT_LENGTH).to(device)
    outputs = model.generate(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=max_length, num_beams=num_beams, early_stopping=True, no_repeat_ngram_size=2)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

smooth = SmoothingFunction().method1
bleu_scores = []
rouge_results = []
eval_sample = pairs_df.sample(n=min(200, len(pairs_df)), random_state=RANDOM_SEED)

with torch.no_grad():
    for i, row in eval_sample.iterrows():
        pred = generate_reply(row['context'], model, tokenizer)
        try:
            bleu = sentence_bleu([row['response'].split()], pred.split(), smoothing_function=smooth)
        except Exception:
            bleu = 0.0
        bleu_scores.append(bleu)
        if _HAS_ROUGE:
            scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)
            sc = scorer.score(row['response'], pred)
            rouge_results.append(sc)

avg_bleu = float(np.mean(bleu_scores)) if bleu_scores else 0.0
if _HAS_ROUGE and rouge_results:
    avg_r1 = np.mean([r['rouge1'].fmeasure for r in rouge_results])
    avg_rl = np.mean([r['rougeL'].fmeasure for r in rouge_results])

try:
    eval_enc = preprocess_pairs(eval_sample, tokenizer)
    eval_dataset = ChatRecDataset(eval_enc)
    eval_res = trainer.evaluate(eval_dataset=eval_dataset)
    eval_loss = eval_res.get('eval_loss', None)
    if eval_loss is not None:
        perplexity = math.exp(eval_loss) if eval_loss < 100 else float('inf')
except Exception:
    eval_loss = None
    perplexity = None

report_txt = os.path.join(OUTPUT_DIR, 'Report.txt')
with open(report_txt, 'w', encoding='utf-8') as f:
    f.write(f"Model: {MODEL_NAME}\nTraining samples: {len(pairs_df)}\nAvg BLEU: {avg_bleu:.4f}\n")
    if eval_loss is not None:
        f.write(f"Eval loss: {eval_loss}\nPerplexity: {perplexity}\n")
    if _HAS_ROUGE and rouge_results:
        f.write(f"Avg ROUGE-1: {avg_r1:.4f}\nAvg ROUGE-L: {avg_rl:.4f}\n")

readme_path = os.path.join(OUTPUT_DIR, 'ReadMe.txt')
with open(readme_path, 'w', encoding='utf-8') as f:
    f.write("1. Ensure dependencies are installed.\n")
    f.write("2. Place datasets at /Desktop/Dataset paths.\n")
    f.write("3. Run the notebook.\n")
    f.write("4. Outputs are saved in ./chatrec_output.\n")
